# PEC Chatbot Backend Environment Configuration
# Copy this file to .env and update with your actual values

# =============================================================================
# REQUIRED API KEYS
# =============================================================================

# Groq API Key for LLM access
# Get your API key from: https://console.groq.com/
# Format: gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
GROQ_API_KEY=your_groq_api_key_here

# Pinecone API Key for vector database
# Get your API key from: https://app.pinecone.io/
# Format: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
PINECONE_API_KEY=your_pinecone_api_key_here

# =============================================================================
# PINECONE CONFIGURATION
# =============================================================================

# Pinecone Index Name
# The name of your Pinecone index (must be 384 dimensions)
# Default: pec-assistant-index
PINECONE_INDEX_NAME=pec-assistant-index

# Pinecone Environment
# Your Pinecone environment name (e.g., us-east-1-aws, us-west1-gcp)
# Get this from your Pinecone dashboard
PINECONE_ENV=your_pinecone_environment

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================

# Log Level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# DEBUG: Detailed logging for development
# INFO: Standard logging for production
LOG_LEVEL=INFO

# Application Mode
# Options: DEVELOPMENT, PRODUCTION
# DEVELOPMENT: Additional debugging and verbose logging
# PRODUCTION: Optimized for performance and security
APP_MODE=PRODUCTION

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Server Host
# IP address to bind the server to
# 0.0.0.0: Bind to all interfaces (for Docker/production)
# 127.0.0.1: Bind to localhost only (for development)
HOST=0.0.0.0

# Server Port
# Port number for the FastAPI server
# Default: 8000
PORT=8000

# =============================================================================
# CORS CONFIGURATION
# =============================================================================

# Allowed Origins
# Comma-separated list of allowed origins for CORS
# Development: http://localhost:5173,http://127.0.0.1:5173
# Production: https://yourdomain.com,https://www.yourdomain.com
CORS_ORIGINS=http://localhost:5173,http://127.0.0.1:5173

# =============================================================================
# SECURITY SETTINGS
# =============================================================================

# Secret Key
# Used for session management and security features
# Generate a random string for production
SECRET_KEY=your_secret_key_here_change_in_production

# Allowed Hosts
# Comma-separated list of allowed hostnames
# For production, specify your domain names
ALLOWED_HOSTS=localhost,127.0.0.1,yourdomain.com

# =============================================================================
# PERFORMANCE SETTINGS
# =============================================================================

# Max Workers
# Number of worker processes for handling requests
# Recommended: 2-4 for most deployments
MAX_WORKERS=4

# Request Timeout
# Maximum time to wait for a request to complete (seconds)
# Default: 30 seconds
TIMEOUT=30

# Max File Size
# Maximum size for uploaded files (e.g., 50MB, 100MB)
# Used for PDF uploads
MAX_FILE_SIZE=50MB

# =============================================================================
# RAG SYSTEM CONFIGURATION
# =============================================================================

# Embedding Model
# HuggingFace model for generating embeddings
# Current: sentence-transformers/all-MiniLM-L6-v2 (384 dimensions)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# LLM Model
# Groq model for generating responses
# Current: llama-3.3-70b-versatile
LLM_MODEL=llama-3.3-70b-versatile

# LLM Temperature
# Controls randomness in responses (0.0 = deterministic, 1.0 = very random)
# Recommended: 0.3 for balanced creativity and accuracy
LLM_TEMPERATURE=0.3

# Max Tokens
# Maximum number of tokens in LLM response
# Default: 1000 tokens
MAX_TOKENS=1000

# =============================================================================
# RETRIEVAL CONFIGURATION
# =============================================================================

# Top K Documents
# Number of documents to retrieve from vector database
# Default: 15 documents
TOP_K_DOCS=15

# Context Documents
# Number of documents to use for context in response generation
# Default: 5 documents
CONTEXT_DOCS=5

# Chunk Size
# Size of text chunks for processing (characters)
# Default: 1000 characters
CHUNK_SIZE=1000

# Chunk Overlap
# Overlap between chunks (characters)
# Default: 200 characters
CHUNK_OVERLAP=200

# =============================================================================
# MONITORING & LOGGING
# =============================================================================

# Enable Request Logging
# Log all incoming requests and responses
# Options: true, false
ENABLE_REQUEST_LOGGING=true

# Log Level for Requests
# Level of detail for request logging
# Options: DEBUG, INFO, WARNING, ERROR
REQUEST_LOG_LEVEL=INFO

# Enable Performance Monitoring
# Track response times and performance metrics
# Options: true, false
ENABLE_PERFORMANCE_MONITORING=true

# =============================================================================
# DEVELOPMENT SETTINGS
# =============================================================================

# Debug Mode
# Enable debug features and verbose error messages
# WARNING: Never enable in production
DEBUG=false

# Reload on Changes
# Automatically reload server when code changes
# Only for development
RELOAD=false

# =============================================================================
# EXTERNAL SERVICES
# =============================================================================

# Enable Health Checks
# Enable health check endpoints for monitoring
# Options: true, false
ENABLE_HEALTH_CHECKS=true

# Health Check Interval
# Interval for health checks (seconds)
# Default: 30 seconds
HEALTH_CHECK_INTERVAL=30

# =============================================================================
# BACKUP & RECOVERY
# =============================================================================

# Enable Data Backup
# Enable automatic backup of configuration and data
# Options: true, false
ENABLE_BACKUP=false

# Backup Interval
# Interval for automatic backups (hours)
# Default: 24 hours
BACKUP_INTERVAL=24

# =============================================================================
# NOTES
# =============================================================================

# 1. Never commit the actual .env file to version control
# 2. Use strong, unique API keys for production
# 3. Rotate API keys regularly for security
# 4. Monitor API usage to avoid rate limits
# 5. Test all configurations in a staging environment first
# 6. Keep backups of your configuration files
# 7. Use environment-specific values for different deployments
